{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "60f7bd48",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Raw data shape: (6612, 31)\n",
      "Columns: ['CASEID', 'AGE', 'AGE_GRP', 'REGION', 'EDUC_LEVEL', 'RELIGION', 'ETHNICITY', 'EDUC', 'HOUSEHOLD_HEAD_SEX', 'PARITY', 'CONTRACEPTIVE_METHOD', 'CURRENT_USE_TYPE', 'LAST_SOURCE_TYPE', 'MONTH_USE_CURRENT_METHOD', 'LAST_METHOD_DISCONTINUED', 'REASON_DISCONTINUED', 'PATTERN_USE', 'INTENTION_USE', 'CONTRACEPTIVE_USE_AND_INTENTION', 'WANT_LAST_CHILD', 'WANT_LAST_PREGNANCY', 'TOLD_ABT_SIDE_EFFECTS', 'SMOKE_CIGAR', 'MARITAL_STATUS', 'RESIDING_WITH_PARTNER', 'DESIRE_FOR_MORE_CHILDREN', 'HSBND_DESIRE_FOR_MORE_CHILDREN', 'OCCUPATION', 'HUSBANDS_EDUC', 'HUSBAND_AGE', 'PARTNER_EDUC']\n",
      "Filtered to current users. Shape: (3521, 31)\n",
      "Target value counts (including NaN):\n",
      "HIGH_RISK_DISCONTINUE\n",
      "1.0    2425\n",
      "0.0    1096\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Design 1 modeling sample shape: (3521, 32)\n",
      "Target distribution (%):\n",
      "HIGH_RISK_DISCONTINUE\n",
      "1    68.87\n",
      "0    31.13\n",
      "Name: proportion, dtype: float64\n",
      "\n",
      "Number of selected features: 25\n",
      "Selected feature columns:\n",
      "['AGE', 'REGION', 'EDUC_LEVEL', 'RELIGION', 'ETHNICITY', 'MARITAL_STATUS', 'RESIDING_WITH_PARTNER', 'HOUSEHOLD_HEAD_SEX', 'OCCUPATION', 'HUSBANDS_EDUC', 'HUSBAND_AGE', 'PARTNER_EDUC', 'SMOKE_CIGAR', 'PARITY', 'DESIRE_FOR_MORE_CHILDREN', 'WANT_LAST_CHILD', 'WANT_LAST_PREGNANCY', 'CONTRACEPTIVE_METHOD', 'MONTH_USE_CURRENT_METHOD', 'PATTERN_USE', 'TOLD_ABT_SIDE_EFFECTS', 'LAST_SOURCE_TYPE', 'LAST_METHOD_DISCONTINUED', 'REASON_DISCONTINUED', 'HSBND_DESIRE_FOR_MORE_CHILDREN']\n",
      "\n",
      "Final X shape: (3521, 25)\n",
      "Final y shape: (3521,)\n",
      "\n",
      "Train shape: (2816, 25) Test shape: (705, 25)\n",
      "Train target distribution (%):\n",
      "HIGH_RISK_DISCONTINUE\n",
      "1    68.86\n",
      "0    31.14\n",
      "Name: proportion, dtype: float64\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'\\nAt this point you have:\\n- df_model_d1: filtered current users with binary target HIGH_RISK_DISCONTINUE\\n- X, y: ready for preprocessing (imputation, encoding) and modeling\\nYou can now plug X_train, X_test, y_train, y_test into your XGBoost + Decision Tree\\nhybrid pipeline defined earlier.\\n'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "Fully specified, error-safe code block for Design 1:\n",
    "\"Among current contraceptive users, classify who is high-risk for discontinuation.\"\n",
    "\n",
    "Assumptions:\n",
    "- Input file: merged_dataset.csv\n",
    "- Key columns exist as in your schema:\n",
    "  CURRENT_USE_TYPE, CONTRACEPTIVE_USE_AND_INTENTION, INTENTION_USE,\n",
    "  LAST_METHOD_DISCONTINUED, REASON_DISCONTINUED, plus demographic/fertility/method covariates.\n",
    "\n",
    "This code:\n",
    "1. Loads data\n",
    "2. Filters to current users\n",
    "3. Constructs a binary target HIGH_RISK_DISCONTINUE\n",
    "4. Selects a feature set with no label leakage\n",
    "5. Prepares X, y for modeling (no model is trained here, so you can plug into your existing pipeline)\n",
    "\n",
    "You can paste this into one or more cells in your notebook and run as-is.\n",
    "\"\"\"\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# -------------------------------------------------------------------\n",
    "# 1. Load data\n",
    "# -------------------------------------------------------------------\n",
    "\n",
    "DATA_PATH = \"../../data/interim/merged_dataset.csv\"  # adjust if needed\n",
    "\n",
    "df = pd.read_csv(DATA_PATH)\n",
    "\n",
    "print(\"Raw data shape:\", df.shape)\n",
    "print(\"Columns:\", list(df.columns))\n",
    "\n",
    "# -------------------------------------------------------------------\n",
    "# 2. Helper: safe membership check on mixed code/label columns\n",
    "# -------------------------------------------------------------------\n",
    "\n",
    "def is_in(series: pd.Series, values) -> pd.Series:\n",
    "    \"\"\"\n",
    "    Robust membership check that works if the column has mixed types\n",
    "    (e.g. numeric codes and string labels).\n",
    "    \"\"\"\n",
    "    values_str = set(str(v) for v in values)\n",
    "    return series.astype(str).isin(values_str)\n",
    "\n",
    "\n",
    "# -------------------------------------------------------------------\n",
    "# 3. Filter to CURRENT users only\n",
    "#    We treat '3' and 'Current user' as current user status.\n",
    "# -------------------------------------------------------------------\n",
    "\n",
    "if \"CURRENT_USE_TYPE\" not in df.columns:\n",
    "    raise ValueError(\"Column 'CURRENT_USE_TYPE' not found in the dataset.\")\n",
    "\n",
    "current_user_values = [\"3\", \"Current user\"]\n",
    "\n",
    "df_current = df[is_in(df[\"CURRENT_USE_TYPE\"], current_user_values)].copy()\n",
    "print(\"Filtered to current users. Shape:\", df_current.shape)\n",
    "\n",
    "# -------------------------------------------------------------------\n",
    "# 4. Construct HIGH_RISK_DISCONTINUE target (Design 1)\n",
    "#    Logic:\n",
    "#    - High risk (1) if:\n",
    "#        * current user AND\n",
    "#        * (intention suggests 'using but intends to stop' OR negative history)\n",
    "#    - Low risk (0) if:\n",
    "#        * current user AND\n",
    "#        * intention suggests 'using and intends to continue'\n",
    "#    - Others: target = NaN (dropped later)\n",
    "# -------------------------------------------------------------------\n",
    "\n",
    "# 4.1 Intention-based risk using CONTRACEPTIVE_USE_AND_INTENTION\n",
    "high_risk_intention_values = [\n",
    "    \"3\",                           # numeric code for \"Using but intends to stop\" (from your description)\n",
    "    \"Using but intends to stop\",   # label form\n",
    "]\n",
    "\n",
    "low_risk_intention_values = [\n",
    "    \"1\",                           # numeric code for \"Using and intends to continue\"\n",
    "    \"Using and intends to continue\"\n",
    "]\n",
    "\n",
    "if \"CONTRACEPTIVE_USE_AND_INTENTION\" in df_current.columns:\n",
    "    intention_high_risk = is_in(df_current[\"CONTRACEPTIVE_USE_AND_INTENTION\"],\n",
    "                                high_risk_intention_values)\n",
    "    intention_low_risk = is_in(df_current[\"CONTRACEPTIVE_USE_AND_INTENTION\"],\n",
    "                               low_risk_intention_values)\n",
    "else:\n",
    "    # If column is missing (should not happen with your schema), default to False\n",
    "    intention_high_risk = pd.Series(False, index=df_current.index)\n",
    "    intention_low_risk = pd.Series(False, index=df_current.index)\n",
    "\n",
    "# 4.2 Additional INTENTION_USE-based risk (fallback if you want to widen coverage)\n",
    "# NOTE: we don't know the exact semantics of numeric codes for INTENTION_USE,\n",
    "# so we only use it as a secondary hint and keep the mapping conservative.\n",
    "\n",
    "if \"INTENTION_USE\" in df_current.columns:\n",
    "    # Example: consider larger numeric codes as \"problematic / no intention / undecided\"\n",
    "    high_risk_INTENTION_USE_values = [\n",
    "        \"4\", \"5\", \"7\",   # codes seen in your sample that might correspond to \"no intention\"/\"undecided\"\n",
    "        \"No intention\",\n",
    "        \"Intends to stop\",\n",
    "        \"Undecided\",\n",
    "    ]\n",
    "    low_risk_INTENTION_USE_values = [\n",
    "        \"1\", \"2\", \"3\",\n",
    "        \"Intends to continue\",\n",
    "        \"Intends to use\",\n",
    "    ]\n",
    "    intention_use_high_risk = is_in(df_current[\"INTENTION_USE\"], high_risk_INTENTION_USE_values)\n",
    "    intention_use_low_risk = is_in(df_current[\"INTENTION_USE\"], low_risk_INTENTION_USE_values)\n",
    "else:\n",
    "    intention_use_high_risk = pd.Series(False, index=df_current.index)\n",
    "    intention_use_low_risk = pd.Series(False, index=df_current.index)\n",
    "\n",
    "# Combine intention signals\n",
    "any_intention_high_risk = intention_high_risk | intention_use_high_risk\n",
    "any_intention_low_risk = intention_low_risk | intention_use_low_risk\n",
    "\n",
    "# 4.3 Negative history:\n",
    "#     To stay error-free and general, we treat any non-empty LAST_METHOD_DISCONTINUED\n",
    "#     with non-empty REASON_DISCONTINUED as a \"negative history\" of discontinuation.\n",
    "if \"LAST_METHOD_DISCONTINUED\" in df_current.columns:\n",
    "    last_method_nonempty = df_current[\"LAST_METHOD_DISCONTINUED\"].astype(str).str.strip() != \"\"\n",
    "else:\n",
    "    last_method_nonempty = pd.Series(False, index=df_current.index)\n",
    "\n",
    "if \"REASON_DISCONTINUED\" in df_current.columns:\n",
    "    reason_nonempty = df_current[\"REASON_DISCONTINUED\"].astype(str).str.strip() != \"\"\n",
    "else:\n",
    "    reason_nonempty = pd.Series(False, index=df_current.index)\n",
    "\n",
    "has_negative_history = last_method_nonempty & reason_nonempty\n",
    "\n",
    "# 4.4 Final target: HIGH_RISK_DISCONTINUE\n",
    "# High risk = 1 if:\n",
    "#   - any_intention_high_risk OR\n",
    "#   - has_negative_history\n",
    "#\n",
    "# Low risk = 0 if:\n",
    "#   - any_intention_low_risk AND NOT high-risk condition\n",
    "#\n",
    "# Others = NaN (unknown)\n",
    "target = np.where(\n",
    "    any_intention_high_risk | has_negative_history,\n",
    "    1,\n",
    "    np.where(\n",
    "        any_intention_low_risk & ~(any_intention_high_risk | has_negative_history),\n",
    "        0,\n",
    "        np.nan\n",
    "    )\n",
    ")\n",
    "\n",
    "df_current[\"HIGH_RISK_DISCONTINUE\"] = target\n",
    "\n",
    "print(\"Target value counts (including NaN):\")\n",
    "print(df_current[\"HIGH_RISK_DISCONTINUE\"].value_counts(dropna=False))\n",
    "\n",
    "# Keep only rows with defined target\n",
    "df_model_d1 = df_current.dropna(subset=[\"HIGH_RISK_DISCONTINUE\"]).copy()\n",
    "df_model_d1[\"HIGH_RISK_DISCONTINUE\"] = df_model_d1[\"HIGH_RISK_DISCONTINUE\"].astype(int)\n",
    "\n",
    "print(\"\\nDesign 1 modeling sample shape:\", df_model_d1.shape)\n",
    "print(\"Target distribution (%):\")\n",
    "print((df_model_d1[\"HIGH_RISK_DISCONTINUE\"].value_counts(normalize=True) * 100).round(2))\n",
    "\n",
    "# -------------------------------------------------------------------\n",
    "# 5. Define feature set (no direct label leakage)\n",
    "#    We EXCLUDE:\n",
    "#      - CONTRACEPTIVE_USE_AND_INTENTION\n",
    "#      - INTENTION_USE\n",
    "#      - HIGH_RISK_DISCONTINUE (the target itself)\n",
    "# -------------------------------------------------------------------\n",
    "\n",
    "feature_cols_demo = [\n",
    "    \"AGE\", \"REGION\", \"EDUC_LEVEL\", \"RELIGION\", \"ETHNICITY\",\n",
    "    \"MARITAL_STATUS\", \"RESIDING_WITH_PARTNER\",\n",
    "    \"HOUSEHOLD_HEAD_SEX\", \"OCCUPATION\",\n",
    "    \"HUSBANDS_EDUC\", \"HUSBAND_AGE\", \"PARTNER_EDUC\",\n",
    "    \"SMOKE_CIGAR\"\n",
    "]\n",
    "\n",
    "feature_cols_fertility = [\n",
    "    \"PARITY\",\n",
    "    \"DESIRE_FOR_MORE_CHILDREN\",\n",
    "    \"WANT_LAST_CHILD\",\n",
    "    \"WANT_LAST_PREGNANCY\",\n",
    "]\n",
    "\n",
    "feature_cols_method = [\n",
    "    \"CONTRACEPTIVE_METHOD\",\n",
    "    \"MONTH_USE_CURRENT_METHOD\",\n",
    "    \"PATTERN_USE\",\n",
    "    \"TOLD_ABT_SIDE_EFFECTS\",\n",
    "    \"LAST_SOURCE_TYPE\",\n",
    "    \"LAST_METHOD_DISCONTINUED\",\n",
    "    \"REASON_DISCONTINUED\",\n",
    "    \"HSBND_DESIRE_FOR_MORE_CHILDREN\",\n",
    "]\n",
    "\n",
    "# Columns that directly define or heavily overlap with the target and should be excluded\n",
    "leakage_cols = [\n",
    "    \"CONTRACEPTIVE_USE_AND_INTENTION\",\n",
    "    \"INTENTION_USE\",\n",
    "    \"HIGH_RISK_DISCONTINUE\",\n",
    "]\n",
    "\n",
    "# Combine and keep only columns that actually exist in df_model_d1 and are not leakage\n",
    "all_candidate_features = feature_cols_demo + feature_cols_fertility + feature_cols_method\n",
    "feature_cols = [\n",
    "    c for c in all_candidate_features\n",
    "    if c in df_model_d1.columns and c not in leakage_cols\n",
    "]\n",
    "\n",
    "print(\"\\nNumber of selected features:\", len(feature_cols))\n",
    "print(\"Selected feature columns:\")\n",
    "print(feature_cols)\n",
    "\n",
    "# -------------------------------------------------------------------\n",
    "# 6. Build X, y and (optionally) train-test split\n",
    "# -------------------------------------------------------------------\n",
    "\n",
    "TARGET_D1 = \"HIGH_RISK_DISCONTINUE\"\n",
    "\n",
    "X = df_model_d1[feature_cols].copy()\n",
    "y = df_model_d1[TARGET_D1].copy()\n",
    "\n",
    "print(\"\\nFinal X shape:\", X.shape)\n",
    "print(\"Final y shape:\", y.shape)\n",
    "\n",
    "# Optional: train-test split (stratified)\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X,\n",
    "    y,\n",
    "    test_size=0.2,\n",
    "    random_state=42,\n",
    "    stratify=y\n",
    ")\n",
    "\n",
    "print(\"\\nTrain shape:\", X_train.shape, \"Test shape:\", X_test.shape)\n",
    "print(\"Train target distribution (%):\")\n",
    "print((y_train.value_counts(normalize=True) * 100).round(2))\n",
    "\n",
    "\"\"\"\n",
    "At this point you have:\n",
    "- df_model_d1: filtered current users with binary target HIGH_RISK_DISCONTINUE\n",
    "- X, y: ready for preprocessing (imputation, encoding) and modeling\n",
    "You can now plug X_train, X_test, y_train, y_test into your XGBoost + Decision Tree\n",
    "hybrid pipeline defined earlier.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "42cc67a9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['../../data/processed/discontinuation_design1_full_data.pkl']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import joblib\n",
    "joblib.dump((X_train, X_test, y_train, y_test), '../../data/processed/discontinuation_design1_data.pkl')\n",
    "joblib.dump(df_model_d1, '../../data/processed/discontinuation_design1_full_data.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8ccad853",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Numeric features: ['AGE', 'PARITY']\n",
      "Categorical features: ['REGION', 'EDUC_LEVEL', 'RELIGION', 'ETHNICITY', 'MARITAL_STATUS', 'RESIDING_WITH_PARTNER', 'HOUSEHOLD_HEAD_SEX', 'OCCUPATION', 'HUSBANDS_EDUC', 'HUSBAND_AGE', 'PARTNER_EDUC', 'SMOKE_CIGAR', 'DESIRE_FOR_MORE_CHILDREN', 'WANT_LAST_CHILD', 'WANT_LAST_PREGNANCY', 'CONTRACEPTIVE_METHOD', 'MONTH_USE_CURRENT_METHOD', 'PATTERN_USE', 'TOLD_ABT_SIDE_EFFECTS', 'LAST_SOURCE_TYPE', 'LAST_METHOD_DISCONTINUED', 'REASON_DISCONTINUED', 'HSBND_DESIRE_FOR_MORE_CHILDREN']\n",
      "\n",
      "=== Training XGBoost baseline ===\n",
      "\n",
      "=== XGBoost performance (HIGH_RISK_DISCONTINUE) ===\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0      1.000     1.000     1.000       219\n",
      "           1      1.000     1.000     1.000       486\n",
      "\n",
      "    accuracy                          1.000       705\n",
      "   macro avg      1.000     1.000     1.000       705\n",
      "weighted avg      1.000     1.000     1.000       705\n",
      "\n",
      "Confusion matrix (XGBoost):\n",
      "[[219   0]\n",
      " [  0 486]]\n",
      "\n",
      "=== Training Decision Tree baseline ===\n",
      "\n",
      "=== Decision Tree performance (HIGH_RISK_DISCONTINUE) ===\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0      1.000     1.000     1.000       219\n",
      "           1      1.000     1.000     1.000       486\n",
      "\n",
      "    accuracy                          1.000       705\n",
      "   macro avg      1.000     1.000     1.000       705\n",
      "weighted avg      1.000     1.000     1.000       705\n",
      "\n",
      "Confusion matrix (Decision Tree):\n",
      "[[219   0]\n",
      " [  0 486]]\n",
      "\n",
      "=== Training hybrid components ===\n",
      "\n",
      "=== Hybrid (XGBoost + Decision Tree) performance ===\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0      1.000     1.000     1.000       219\n",
      "           1      1.000     1.000     1.000       486\n",
      "\n",
      "    accuracy                          1.000       705\n",
      "   macro avg      1.000     1.000     1.000       705\n",
      "weighted avg      1.000     1.000     1.000       705\n",
      "\n",
      "Confusion matrix (Hybrid):\n",
      "[[219   0]\n",
      " [  0 486]]\n",
      "\n",
      "Fraction of test cases overridden by Decision Tree: 0.00%\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'\\nAt this point you have:\\n- xgb_pipeline, dt_pipeline: standalone models for HIGH_RISK_DISCONTINUE\\n- xgb_pipeline_hybrid, dt_pipeline_hybrid, and hybrid_pred:\\n  a hybrid system where Decision Tree helps when XGBoost is uncertain.\\n\\nYou can now:\\n- Tune XGBoost hyperparameters (n_estimators, max_depth, learning_rate, scale_pos_weight).\\n- Tune DecisionTree (max_depth, min_samples_leaf).\\n- Tune CONF_MARGIN to trade off:\\n    * reliance on XGBoost vs DT\\n    * precision vs recall for the high-risk class.\\n- Save the pipelines with joblib for deployment.\\n'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "Hybrid model training for Design 1 (HIGH_RISK_DISCONTINUE):\n",
    "- Uses X_train, X_test, y_train, y_test from the previous block.\n",
    "- Builds a preprocessing pipeline (impute + one-hot).\n",
    "- Trains:\n",
    "    1) XGBoost classifier\n",
    "    2) Decision Tree classifier\n",
    "    3) Hybrid: XGBoost with Decision Tree override on low-confidence cases.\n",
    "\"\"\"\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import OneHotEncoder, LabelEncoder\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "# -------------------------------------------------------------------\n",
    "# 1. Identify categorical vs numeric features\n",
    "# -------------------------------------------------------------------\n",
    "\n",
    "# X_train already defined; use its dtypes\n",
    "all_features = list(X_train.columns)\n",
    "\n",
    "# Simple rule: numeric dtypes -> numeric; others -> categorical\n",
    "numeric_features = [\n",
    "    col for col in all_features\n",
    "    if pd.api.types.is_numeric_dtype(X_train[col])\n",
    "]\n",
    "\n",
    "categorical_features = [\n",
    "    col for col in all_features\n",
    "    if col not in numeric_features\n",
    "]\n",
    "\n",
    "print(\"Numeric features:\", numeric_features)\n",
    "print(\"Categorical features:\", categorical_features)\n",
    "\n",
    "# -------------------------------------------------------------------\n",
    "# 2. Define preprocessing: imputation + encoding\n",
    "# -------------------------------------------------------------------\n",
    "\n",
    "numeric_transformer = Pipeline(steps=[\n",
    "    (\"imputer\", SimpleImputer(strategy=\"median\")),\n",
    "])\n",
    "\n",
    "categorical_transformer = Pipeline(steps=[\n",
    "    (\"imputer\", SimpleImputer(strategy=\"most_frequent\")),\n",
    "    (\"onehot\", OneHotEncoder(handle_unknown=\"ignore\")),\n",
    "])\n",
    "\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        (\"num\", numeric_transformer, numeric_features),\n",
    "        (\"cat\", categorical_transformer, categorical_features),\n",
    "    ]\n",
    ")\n",
    "\n",
    "# -------------------------------------------------------------------\n",
    "# 3. XGBoost baseline pipeline\n",
    "# -------------------------------------------------------------------\n",
    "\n",
    "xgb_clf = XGBClassifier(\n",
    "    n_estimators=300,\n",
    "    max_depth=5,\n",
    "    learning_rate=0.05,\n",
    "    subsample=0.8,\n",
    "    colsample_bytree=0.8,\n",
    "    objective=\"binary:logistic\",\n",
    "    tree_method=\"hist\",\n",
    "    eval_metric=\"logloss\",\n",
    "    random_state=42,\n",
    "    scale_pos_weight=None  # you can tune using class imbalance ratio\n",
    ")\n",
    "\n",
    "xgb_pipeline = Pipeline(steps=[\n",
    "    (\"preprocess\", preprocessor),\n",
    "    (\"model\", xgb_clf)\n",
    "])\n",
    "\n",
    "print(\"\\n=== Training XGBoost baseline ===\")\n",
    "xgb_pipeline.fit(X_train, y_train)\n",
    "y_pred_xgb = xgb_pipeline.predict(X_test)\n",
    "\n",
    "print(\"\\n=== XGBoost performance (HIGH_RISK_DISCONTINUE) ===\")\n",
    "print(classification_report(y_test, y_pred_xgb, digits=3))\n",
    "print(\"Confusion matrix (XGBoost):\")\n",
    "print(confusion_matrix(y_test, y_pred_xgb))\n",
    "\n",
    "# -------------------------------------------------------------------\n",
    "# 4. Decision Tree baseline pipeline\n",
    "# -------------------------------------------------------------------\n",
    "\n",
    "dt_clf = DecisionTreeClassifier(\n",
    "    max_depth=5,\n",
    "    min_samples_leaf=50,\n",
    "    random_state=42,\n",
    "    class_weight=\"balanced\"  # helps with imbalance\n",
    ")\n",
    "\n",
    "dt_pipeline = Pipeline(steps=[\n",
    "    (\"preprocess\", preprocessor),\n",
    "    (\"model\", dt_clf)\n",
    "])\n",
    "\n",
    "print(\"\\n=== Training Decision Tree baseline ===\")\n",
    "dt_pipeline.fit(X_train, y_train)\n",
    "y_pred_dt = dt_pipeline.predict(X_test)\n",
    "\n",
    "print(\"\\n=== Decision Tree performance (HIGH_RISK_DISCONTINUE) ===\")\n",
    "print(classification_report(y_test, y_pred_dt, digits=3))\n",
    "print(\"Confusion matrix (Decision Tree):\")\n",
    "print(confusion_matrix(y_test, y_pred_dt))\n",
    "\n",
    "# -------------------------------------------------------------------\n",
    "# 5. Hybrid model: XGBoost + Decision Tree override\n",
    "#    Strategy:\n",
    "#      - Use XGBoost as main model (probabilistic).\n",
    "#      - When XGBoost's predicted probability is low-confidence,\n",
    "#        override with Decision Tree prediction.\n",
    "# -------------------------------------------------------------------\n",
    "\n",
    "# For binary classification, XGBClassifier already returns proba for positive class\n",
    "# through predict_proba(...)[..., 1].\n",
    "\n",
    "print(\"\\n=== Training hybrid components ===\")\n",
    "\n",
    "# Reuse the same preprocessor, but to be explicit we build new pipelines\n",
    "# (you can reuse xgb_pipeline and dt_pipeline if you prefer).\n",
    "\n",
    "xgb_hybrid = XGBClassifier(\n",
    "    n_estimators=300,\n",
    "    max_depth=5,\n",
    "    learning_rate=0.05,\n",
    "    subsample=0.8,\n",
    "    colsample_bytree=0.8,\n",
    "    objective=\"binary:logistic\",\n",
    "    tree_method=\"hist\",\n",
    "    eval_metric=\"logloss\",\n",
    "    random_state=42,\n",
    ")\n",
    "\n",
    "xgb_pipeline_hybrid = Pipeline(steps=[\n",
    "    (\"preprocess\", preprocessor),\n",
    "    (\"model\", xgb_hybrid)\n",
    "])\n",
    "\n",
    "dt_hybrid = DecisionTreeClassifier(\n",
    "    max_depth=5,\n",
    "    min_samples_leaf=50,\n",
    "    random_state=42,\n",
    "    class_weight=\"balanced\"\n",
    ")\n",
    "\n",
    "dt_pipeline_hybrid = Pipeline(steps=[\n",
    "    (\"preprocess\", preprocessor),\n",
    "    (\"model\", dt_hybrid)\n",
    "])\n",
    "\n",
    "# Fit both on the same train set\n",
    "xgb_pipeline_hybrid.fit(X_train, y_train)\n",
    "dt_pipeline_hybrid.fit(X_train, y_train)\n",
    "\n",
    "# XGBoost probabilities and baseline prediction\n",
    "proba_xgb = xgb_pipeline_hybrid.predict_proba(X_test)[:, 1]  # P(y=1)\n",
    "pred_xgb = (proba_xgb >= 0.5).astype(int)\n",
    "\n",
    "# Decision Tree prediction\n",
    "pred_dt = dt_pipeline_hybrid.predict(X_test)\n",
    "\n",
    "# Hybrid rule:\n",
    "# - If XGBoost confidence (distance to 0.5) is low, trust DT instead.\n",
    "#   For binary, we can say \"confidence\" = |p - 0.5|.\n",
    "#   If confidence < CONF_MARGIN, we override by DT.\n",
    "\n",
    "CONF_MARGIN = 0.1  # you can tune; smaller = fewer overrides\n",
    "confidence_xgb = np.abs(proba_xgb - 0.5)\n",
    "\n",
    "use_dt = confidence_xgb < CONF_MARGIN\n",
    "hybrid_pred = np.where(use_dt, pred_dt, pred_xgb)\n",
    "\n",
    "print(\"\\n=== Hybrid (XGBoost + Decision Tree) performance ===\")\n",
    "print(classification_report(y_test, hybrid_pred, digits=3))\n",
    "print(\"Confusion matrix (Hybrid):\")\n",
    "print(confusion_matrix(y_test, hybrid_pred))\n",
    "\n",
    "# Optional: inspect how often the DT override is used\n",
    "override_rate = use_dt.mean() * 100\n",
    "print(f\"\\nFraction of test cases overridden by Decision Tree: {override_rate:.2f}%\")\n",
    "\n",
    "\"\"\"\n",
    "At this point you have:\n",
    "- xgb_pipeline, dt_pipeline: standalone models for HIGH_RISK_DISCONTINUE\n",
    "- xgb_pipeline_hybrid, dt_pipeline_hybrid, and hybrid_pred:\n",
    "  a hybrid system where Decision Tree helps when XGBoost is uncertain.\n",
    "\n",
    "You can now:\n",
    "- Tune XGBoost hyperparameters (n_estimators, max_depth, learning_rate, scale_pos_weight).\n",
    "- Tune DecisionTree (max_depth, min_samples_leaf).\n",
    "- Tune CONF_MARGIN to trade off:\n",
    "    * reliance on XGBoost vs DT\n",
    "    * precision vs recall for the high-risk class.\n",
    "- Save the pipelines with joblib for deployment.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "57843904",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['../models/discontinuation_design1_dt_model_d1.pkl']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "joblib.dump(xgb_pipeline_hybrid, '../models/discontinuation_design1_xgb_hybrid_model_d1.pkl')\n",
    "joblib.dump(dt_pipeline_hybrid, '../models/discontinuation_design1_dt_hybrid_model_d1.pkl')\n",
    "joblib.dump(xgb_pipeline, '../models/discontinuation_design1_xgb_model_d1.pkl')\n",
    "joblib.dump(dt_pipeline, '../models/discontinuation_design1_dt_model_d1.pkl')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.14.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

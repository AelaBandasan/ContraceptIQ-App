{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "59517b77",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Raw data shape: (6612, 31)\n",
      "Columns: ['CASEID', 'AGE', 'AGE_GRP', 'REGION', 'EDUC_LEVEL', 'RELIGION', 'ETHNICITY', 'EDUC', 'HOUSEHOLD_HEAD_SEX', 'PARITY', 'CONTRACEPTIVE_METHOD', 'CURRENT_USE_TYPE', 'LAST_SOURCE_TYPE', 'MONTH_USE_CURRENT_METHOD', 'LAST_METHOD_DISCONTINUED', 'REASON_DISCONTINUED', 'PATTERN_USE', 'INTENTION_USE', 'CONTRACEPTIVE_USE_AND_INTENTION', 'WANT_LAST_CHILD', 'WANT_LAST_PREGNANCY', 'TOLD_ABT_SIDE_EFFECTS', 'SMOKE_CIGAR', 'MARITAL_STATUS', 'RESIDING_WITH_PARTNER', 'DESIRE_FOR_MORE_CHILDREN', 'HSBND_DESIRE_FOR_MORE_CHILDREN', 'OCCUPATION', 'HUSBANDS_EDUC', 'HUSBAND_AGE', 'PARTNER_EDUC']\n",
      "Filtered to current users. Shape: (3521, 31)\n",
      "\n",
      "Target value counts (including NaN):\n",
      "HIGH_RISK_DISCONTINUE\n",
      "0.0    2999\n",
      "NaN     316\n",
      "1.0     206\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Design 1 modeling sample shape: (3205, 32)\n",
      "Target distribution (%):\n",
      "HIGH_RISK_DISCONTINUE\n",
      "0    93.57\n",
      "1     6.43\n",
      "Name: proportion, dtype: float64\n",
      "\n",
      "Number of selected features: 25\n",
      "Selected feature columns:\n",
      "['AGE', 'REGION', 'EDUC_LEVEL', 'RELIGION', 'ETHNICITY', 'MARITAL_STATUS', 'RESIDING_WITH_PARTNER', 'HOUSEHOLD_HEAD_SEX', 'OCCUPATION', 'HUSBANDS_EDUC', 'HUSBAND_AGE', 'PARTNER_EDUC', 'SMOKE_CIGAR', 'PARITY', 'DESIRE_FOR_MORE_CHILDREN', 'WANT_LAST_CHILD', 'WANT_LAST_PREGNANCY', 'CONTRACEPTIVE_METHOD', 'MONTH_USE_CURRENT_METHOD', 'PATTERN_USE', 'TOLD_ABT_SIDE_EFFECTS', 'LAST_SOURCE_TYPE', 'LAST_METHOD_DISCONTINUED', 'REASON_DISCONTINUED', 'HSBND_DESIRE_FOR_MORE_CHILDREN']\n",
      "\n",
      "Final X shape: (3205, 25)\n",
      "Final y shape: (3205,)\n",
      "\n",
      "Train shape: (2564, 25) Test shape: (641, 25)\n",
      "Train target distribution (%):\n",
      "HIGH_RISK_DISCONTINUE\n",
      "0    93.56\n",
      "1     6.44\n",
      "Name: proportion, dtype: float64\n",
      "\n",
      "Numeric features: ['AGE', 'PARITY']\n",
      "Categorical features: ['REGION', 'EDUC_LEVEL', 'RELIGION', 'ETHNICITY', 'MARITAL_STATUS', 'RESIDING_WITH_PARTNER', 'HOUSEHOLD_HEAD_SEX', 'OCCUPATION', 'HUSBANDS_EDUC', 'HUSBAND_AGE', 'PARTNER_EDUC', 'SMOKE_CIGAR', 'DESIRE_FOR_MORE_CHILDREN', 'WANT_LAST_CHILD', 'WANT_LAST_PREGNANCY', 'CONTRACEPTIVE_METHOD', 'MONTH_USE_CURRENT_METHOD', 'PATTERN_USE', 'TOLD_ABT_SIDE_EFFECTS', 'LAST_SOURCE_TYPE', 'LAST_METHOD_DISCONTINUED', 'REASON_DISCONTINUED', 'HSBND_DESIRE_FOR_MORE_CHILDREN']\n",
      "\n",
      "=== Training XGBoost baseline ===\n",
      "\n",
      "=== XGBoost performance (HIGH_RISK_DISCONTINUE) ===\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0      0.941     0.985     0.963       600\n",
      "           1      0.308     0.098     0.148        41\n",
      "\n",
      "    accuracy                          0.928       641\n",
      "   macro avg      0.624     0.541     0.555       641\n",
      "weighted avg      0.901     0.928     0.910       641\n",
      "\n",
      "Confusion matrix (XGBoost):\n",
      "[[591   9]\n",
      " [ 37   4]]\n",
      "\n",
      "=== Training Decision Tree baseline ===\n",
      "\n",
      "=== Decision Tree performance (HIGH_RISK_DISCONTINUE) ===\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0      1.000     0.797     0.887       600\n",
      "           1      0.252     1.000     0.402        41\n",
      "\n",
      "    accuracy                          0.810       641\n",
      "   macro avg      0.626     0.898     0.644       641\n",
      "weighted avg      0.952     0.810     0.856       641\n",
      "\n",
      "Confusion matrix (Decision Tree):\n",
      "[[478 122]\n",
      " [  0  41]]\n",
      "\n",
      "=== Training hybrid components ===\n",
      "\n",
      "=== Hybrid (XGBoost + Decision Tree) performance ===\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0      0.944     0.958     0.951       600\n",
      "           1      0.219     0.171     0.192        41\n",
      "\n",
      "    accuracy                          0.908       641\n",
      "   macro avg      0.581     0.565     0.571       641\n",
      "weighted avg      0.898     0.908     0.903       641\n",
      "\n",
      "Confusion matrix (Hybrid):\n",
      "[[575  25]\n",
      " [ 34   7]]\n",
      "\n",
      "Fraction of test cases overridden by Decision Tree: 4.21%\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'\\nYou now have:\\n- df_model_d1: current users with intention-based HIGH_RISK_DISCONTINUE\\n- X_train, X_test, y_train, y_test\\n- xgb_pipeline, dt_pipeline, and a hybrid model\\n\\nNext steps:\\n- Inspect especially recall/precision for class 1 (high-risk).\\n- Tune XGBoost, Decision Tree, and CONF_MARGIN for your desired trade-off.\\n'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "Design 1 (updated, no leakage from history into label):\n",
    "- Population: CURRENT contraceptive users\n",
    "- Target: HIGH_RISK_DISCONTINUE, defined ONLY from intention variables\n",
    "- Features: demographics, fertility, method characteristics, history (as predictors only)\n",
    "\"\"\"\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "# -------------------------------------------------------------------\n",
    "# 1. Load data\n",
    "# -------------------------------------------------------------------\n",
    "\n",
    "DATA_PATH = \"../../data/interim/merged_dataset.csv\"  # adjust if needed\n",
    "df = pd.read_csv(DATA_PATH)\n",
    "\n",
    "print(\"Raw data shape:\", df.shape)\n",
    "print(\"Columns:\", list(df.columns))\n",
    "\n",
    "# -------------------------------------------------------------------\n",
    "# 2. Helper: safe membership check on mixed code/label columns\n",
    "# -------------------------------------------------------------------\n",
    "\n",
    "def is_in(series: pd.Series, values) -> pd.Series:\n",
    "    \"\"\"\n",
    "    Robust membership check that works if the column has mixed types\n",
    "    (e.g. numeric codes and string labels).\n",
    "    \"\"\"\n",
    "    values_str = set(str(v) for v in values)\n",
    "    return series.astype(str).isin(values_str)\n",
    "\n",
    "# -------------------------------------------------------------------\n",
    "# 3. Filter to CURRENT users only\n",
    "#    Treat '3' and 'Current user' as current user status.\n",
    "# -------------------------------------------------------------------\n",
    "\n",
    "if \"CURRENT_USE_TYPE\" not in df.columns:\n",
    "    raise ValueError(\"Column 'CURRENT_USE_TYPE' not found in the dataset.\")\n",
    "\n",
    "current_user_values = [\"3\", \"Current user\"]\n",
    "df_current = df[is_in(df[\"CURRENT_USE_TYPE\"], current_user_values)].copy()\n",
    "\n",
    "print(\"Filtered to current users. Shape:\", df_current.shape)\n",
    "\n",
    "# -------------------------------------------------------------------\n",
    "# 4. Construct HIGH_RISK_DISCONTINUE target (intention-based only)\n",
    "#    Logic:\n",
    "#    - High risk (1) if intention suggests \"using but intends to stop\" or similar\n",
    "#    - Low risk (0) if intention suggests \"using and intends to continue\"\n",
    "#    - Others => NaN (dropped later)\n",
    "# -------------------------------------------------------------------\n",
    "\n",
    "# 4.1 Intention-based risk using CONTRACEPTIVE_USE_AND_INTENTION\n",
    "if \"CONTRACEPTIVE_USE_AND_INTENTION\" in df_current.columns:\n",
    "    high_risk_intention_values = [\n",
    "        \"3\",                           # numeric code (example) for \"Using but intends to stop\"\n",
    "        \"Using but intends to stop\",\n",
    "        \"Using but unsure\",\n",
    "    ]\n",
    "    low_risk_intention_values = [\n",
    "        \"1\",                           # numeric code (example) for \"Using and intends to continue\"\n",
    "        \"Using and intends to continue\",\n",
    "    ]\n",
    "    intention_high_risk = is_in(df_current[\"CONTRACEPTIVE_USE_AND_INTENTION\"],\n",
    "                                high_risk_intention_values)\n",
    "    intention_low_risk = is_in(df_current[\"CONTRACEPTIVE_USE_AND_INTENTION\"],\n",
    "                               low_risk_intention_values)\n",
    "else:\n",
    "    intention_high_risk = pd.Series(False, index=df_current.index)\n",
    "    intention_low_risk = pd.Series(False, index=df_current.index)\n",
    "\n",
    "# 4.2 Additional INTENTION_USE-based risk (secondary hint)\n",
    "if \"INTENTION_USE\" in df_current.columns:\n",
    "    high_risk_INTENTION_USE_values = [\n",
    "        \"4\", \"5\", \"7\",   # example codes: \"no intention\"/\"undecided\"/\"intends to stop\"\n",
    "        \"No intention\",\n",
    "        \"Intends to stop\",\n",
    "        \"Undecided\",\n",
    "    ]\n",
    "    low_risk_INTENTION_USE_values = [\n",
    "        \"1\", \"2\", \"3\",\n",
    "        \"Intends to continue\",\n",
    "        \"Intends to use\",\n",
    "    ]\n",
    "    intention_use_high_risk = is_in(df_current[\"INTENTION_USE\"],\n",
    "                                    high_risk_INTENTION_USE_values)\n",
    "    intention_use_low_risk = is_in(df_current[\"INTENTION_USE\"],\n",
    "                                   low_risk_INTENTION_USE_values)\n",
    "else:\n",
    "    intention_use_high_risk = pd.Series(False, index=df_current.index)\n",
    "    intention_use_low_risk = pd.Series(False, index=df_current.index)\n",
    "\n",
    "# Combine intention signals (ONLY intentions, no history in label)\n",
    "any_intention_high_risk = intention_high_risk | intention_use_high_risk\n",
    "any_intention_low_risk = intention_low_risk | intention_use_low_risk\n",
    "\n",
    "# 4.3 Final target: HIGH_RISK_DISCONTINUE (intention-based)\n",
    "target = np.where(\n",
    "    any_intention_high_risk,\n",
    "    1,\n",
    "    np.where(\n",
    "        any_intention_low_risk & ~any_intention_high_risk,\n",
    "        0,\n",
    "        np.nan\n",
    "    )\n",
    ")\n",
    "\n",
    "df_current[\"HIGH_RISK_DISCONTINUE\"] = target\n",
    "\n",
    "print(\"\\nTarget value counts (including NaN):\")\n",
    "print(df_current[\"HIGH_RISK_DISCONTINUE\"].value_counts(dropna=False))\n",
    "\n",
    "# Keep only rows with defined target\n",
    "df_model_d1 = df_current.dropna(subset=[\"HIGH_RISK_DISCONTINUE\"]).copy()\n",
    "df_model_d1[\"HIGH_RISK_DISCONTINUE\"] = df_model_d1[\"HIGH_RISK_DISCONTINUE\"].astype(int)\n",
    "\n",
    "print(\"\\nDesign 1 modeling sample shape:\", df_model_d1.shape)\n",
    "print(\"Target distribution (%):\")\n",
    "print((df_model_d1[\"HIGH_RISK_DISCONTINUE\"].value_counts(normalize=True) * 100).round(2))\n",
    "\n",
    "# -------------------------------------------------------------------\n",
    "# 5. Define feature set (no intention columns to avoid label leakage)\n",
    "#    We allow history variables (LAST_METHOD_DISCONTINUED, REASON_DISCONTINUED)\n",
    "#    as predictors, but they are NOT used in the label definition.\n",
    "# -------------------------------------------------------------------\n",
    "\n",
    "feature_cols_demo = [\n",
    "    \"AGE\", \"REGION\", \"EDUC_LEVEL\", \"RELIGION\", \"ETHNICITY\",\n",
    "    \"MARITAL_STATUS\", \"RESIDING_WITH_PARTNER\",\n",
    "    \"HOUSEHOLD_HEAD_SEX\", \"OCCUPATION\",\n",
    "    \"HUSBANDS_EDUC\", \"HUSBAND_AGE\", \"PARTNER_EDUC\",\n",
    "    \"SMOKE_CIGAR\"\n",
    "]\n",
    "\n",
    "feature_cols_fertility = [\n",
    "    \"PARITY\",\n",
    "    \"DESIRE_FOR_MORE_CHILDREN\",\n",
    "    \"WANT_LAST_CHILD\",\n",
    "    \"WANT_LAST_PREGNANCY\",\n",
    "]\n",
    "\n",
    "feature_cols_method = [\n",
    "    \"CONTRACEPTIVE_METHOD\",\n",
    "    \"MONTH_USE_CURRENT_METHOD\",\n",
    "    \"PATTERN_USE\",\n",
    "    \"TOLD_ABT_SIDE_EFFECTS\",\n",
    "    \"LAST_SOURCE_TYPE\",\n",
    "    \"LAST_METHOD_DISCONTINUED\",      # history as feature (OK now)\n",
    "    \"REASON_DISCONTINUED\",           # history as feature (OK now)\n",
    "    \"HSBND_DESIRE_FOR_MORE_CHILDREN\",\n",
    "]\n",
    "\n",
    "# Columns to exclude from features (direct definition of target)\n",
    "leakage_cols = [\n",
    "    \"CONTRACEPTIVE_USE_AND_INTENTION\",\n",
    "    \"INTENTION_USE\",\n",
    "    \"HIGH_RISK_DISCONTINUE\",\n",
    "]\n",
    "\n",
    "all_candidate_features = feature_cols_demo + feature_cols_fertility + feature_cols_method\n",
    "feature_cols = [\n",
    "    c for c in all_candidate_features\n",
    "    if c in df_model_d1.columns and c not in leakage_cols\n",
    "]\n",
    "\n",
    "print(\"\\nNumber of selected features:\", len(feature_cols))\n",
    "print(\"Selected feature columns:\")\n",
    "print(feature_cols)\n",
    "\n",
    "TARGET_D1 = \"HIGH_RISK_DISCONTINUE\"\n",
    "X = df_model_d1[feature_cols].copy()\n",
    "y = df_model_d1[TARGET_D1].copy()\n",
    "\n",
    "print(\"\\nFinal X shape:\", X.shape)\n",
    "print(\"Final y shape:\", y.shape)\n",
    "\n",
    "# -------------------------------------------------------------------\n",
    "# 6. Train-test split (stratified)\n",
    "# -------------------------------------------------------------------\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X,\n",
    "    y,\n",
    "    test_size=0.2,\n",
    "    random_state=42,\n",
    "    stratify=y\n",
    ")\n",
    "\n",
    "print(\"\\nTrain shape:\", X_train.shape, \"Test shape:\", X_test.shape)\n",
    "print(\"Train target distribution (%):\")\n",
    "print((y_train.value_counts(normalize=True) * 100).round(2))\n",
    "\n",
    "# -------------------------------------------------------------------\n",
    "# 7. Identify numeric vs categorical features\n",
    "# -------------------------------------------------------------------\n",
    "\n",
    "all_features = list(X_train.columns)\n",
    "numeric_features = [\n",
    "    col for col in all_features\n",
    "    if pd.api.types.is_numeric_dtype(X_train[col])\n",
    "]\n",
    "categorical_features = [\n",
    "    col for col in all_features\n",
    "    if col not in numeric_features\n",
    "]\n",
    "\n",
    "print(\"\\nNumeric features:\", numeric_features)\n",
    "print(\"Categorical features:\", categorical_features)\n",
    "\n",
    "# -------------------------------------------------------------------\n",
    "# 8. Preprocessing: imputation + one-hot encoding\n",
    "# -------------------------------------------------------------------\n",
    "\n",
    "numeric_transformer = Pipeline(steps=[\n",
    "    (\"imputer\", SimpleImputer(strategy=\"median\")),\n",
    "])\n",
    "\n",
    "categorical_transformer = Pipeline(steps=[\n",
    "    (\"imputer\", SimpleImputer(strategy=\"most_frequent\")),\n",
    "    (\"onehot\", OneHotEncoder(handle_unknown=\"ignore\")),\n",
    "])\n",
    "\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        (\"num\", numeric_transformer, numeric_features),\n",
    "        (\"cat\", categorical_transformer, categorical_features),\n",
    "    ]\n",
    ")\n",
    "\n",
    "# -------------------------------------------------------------------\n",
    "# 9. Train XGBoost baseline\n",
    "# -------------------------------------------------------------------\n",
    "\n",
    "xgb_clf = XGBClassifier(\n",
    "    n_estimators=300,\n",
    "    max_depth=5,\n",
    "    learning_rate=0.05,\n",
    "    subsample=0.8,\n",
    "    colsample_bytree=0.8,\n",
    "    objective=\"binary:logistic\",\n",
    "    tree_method=\"hist\",\n",
    "    eval_metric=\"logloss\",\n",
    "    random_state=42,\n",
    ")\n",
    "\n",
    "xgb_pipeline = Pipeline(steps=[\n",
    "    (\"preprocess\", preprocessor),\n",
    "    (\"model\", xgb_clf)\n",
    "])\n",
    "\n",
    "print(\"\\n=== Training XGBoost baseline ===\")\n",
    "xgb_pipeline.fit(X_train, y_train)\n",
    "y_pred_xgb = xgb_pipeline.predict(X_test)\n",
    "\n",
    "print(\"\\n=== XGBoost performance (HIGH_RISK_DISCONTINUE) ===\")\n",
    "print(classification_report(y_test, y_pred_xgb, digits=3))\n",
    "print(\"Confusion matrix (XGBoost):\")\n",
    "print(confusion_matrix(y_test, y_pred_xgb))\n",
    "\n",
    "# -------------------------------------------------------------------\n",
    "# 10. Train Decision Tree baseline\n",
    "# -------------------------------------------------------------------\n",
    "\n",
    "dt_clf = DecisionTreeClassifier(\n",
    "    max_depth=5,\n",
    "    min_samples_leaf=50,\n",
    "    random_state=42,\n",
    "    class_weight=\"balanced\"\n",
    ")\n",
    "\n",
    "dt_pipeline = Pipeline(steps=[\n",
    "    (\"preprocess\", preprocessor),\n",
    "    (\"model\", dt_clf)\n",
    "])\n",
    "\n",
    "print(\"\\n=== Training Decision Tree baseline ===\")\n",
    "dt_pipeline.fit(X_train, y_train)\n",
    "y_pred_dt = dt_pipeline.predict(X_test)\n",
    "\n",
    "print(\"\\n=== Decision Tree performance (HIGH_RISK_DISCONTINUE) ===\")\n",
    "print(classification_report(y_test, y_pred_dt, digits=3))\n",
    "print(\"Confusion matrix (Decision Tree):\")\n",
    "print(confusion_matrix(y_test, y_pred_dt))\n",
    "\n",
    "# -------------------------------------------------------------------\n",
    "# 11. Hybrid model: XGBoost + Decision Tree override on low-confidence cases\n",
    "# -------------------------------------------------------------------\n",
    "\n",
    "print(\"\\n=== Training hybrid components ===\")\n",
    "\n",
    "xgb_hybrid = XGBClassifier(\n",
    "    n_estimators=300,\n",
    "    max_depth=5,\n",
    "    learning_rate=0.05,\n",
    "    subsample=0.8,\n",
    "    colsample_bytree=0.8,\n",
    "    objective=\"binary:logistic\",\n",
    "    tree_method=\"hist\",\n",
    "    eval_metric=\"logloss\",\n",
    "    random_state=42,\n",
    ")\n",
    "\n",
    "xgb_pipeline_hybrid = Pipeline(steps=[\n",
    "    (\"preprocess\", preprocessor),\n",
    "    (\"model\", xgb_hybrid)\n",
    "])\n",
    "\n",
    "dt_hybrid = DecisionTreeClassifier(\n",
    "    max_depth=5,\n",
    "    min_samples_leaf=50,\n",
    "    random_state=42,\n",
    "    class_weight=\"balanced\"\n",
    ")\n",
    "\n",
    "dt_pipeline_hybrid = Pipeline(steps=[\n",
    "    (\"preprocess\", preprocessor),\n",
    "    (\"model\", dt_hybrid)\n",
    "])\n",
    "\n",
    "xgb_pipeline_hybrid.fit(X_train, y_train)\n",
    "dt_pipeline_hybrid.fit(X_train, y_train)\n",
    "\n",
    "# Probabilities from XGBoost\n",
    "proba_xgb = xgb_pipeline_hybrid.predict_proba(X_test)[:, 1]  # P(y=1)\n",
    "pred_xgb = (proba_xgb >= 0.5).astype(int)\n",
    "\n",
    "# Predictions from Decision Tree\n",
    "pred_dt = dt_pipeline_hybrid.predict(X_test)\n",
    "\n",
    "# Confidence margin for hybrid switching\n",
    "CONF_MARGIN = 0.1  # tune this: smaller = fewer overrides\n",
    "confidence_xgb = np.abs(proba_xgb - 0.5)\n",
    "use_dt = confidence_xgb < CONF_MARGIN\n",
    "\n",
    "hybrid_pred = np.where(use_dt, pred_dt, pred_xgb)\n",
    "\n",
    "print(\"\\n=== Hybrid (XGBoost + Decision Tree) performance ===\")\n",
    "print(classification_report(y_test, hybrid_pred, digits=3))\n",
    "print(\"Confusion matrix (Hybrid):\")\n",
    "print(confusion_matrix(y_test, hybrid_pred))\n",
    "\n",
    "override_rate = use_dt.mean() * 100\n",
    "print(f\"\\nFraction of test cases overridden by Decision Tree: {override_rate:.2f}%\")\n",
    "\n",
    "\"\"\"\n",
    "You now have:\n",
    "- df_model_d1: current users with intention-based HIGH_RISK_DISCONTINUE\n",
    "- X_train, X_test, y_train, y_test\n",
    "- xgb_pipeline, dt_pipeline, and a hybrid model\n",
    "\n",
    "Next steps:\n",
    "- Inspect especially recall/precision for class 1 (high-risk).\n",
    "- Tune XGBoost, Decision Tree, and CONF_MARGIN for your desired trade-off.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "58386318",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['../models/discontinuation_design1_dt_model_d1_v2.pkl']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import joblib\n",
    "joblib.dump((X_train, X_test, y_train, y_test), '../../data/processed/discontinuation_design1_data_v2.pkl')\n",
    "joblib.dump(df_model_d1, '../../data/processed/discontinuation_design1_full_data_v2.pkl')\n",
    "joblib.dump(xgb_pipeline_hybrid, '../models/discontinuation_design1_xgb_hybrid_model_d1_v2.pkl')\n",
    "joblib.dump(dt_pipeline_hybrid, '../models/discontinuation_design1_dt_hybrid_model_d1_v2.pkl')\n",
    "joblib.dump(xgb_pipeline, '../models/discontinuation_design1_xgb_model_d1_v2.pkl')\n",
    "joblib.dump(dt_pipeline, '../models/discontinuation_design1_dt_model_d1_v2.pkl')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.14.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

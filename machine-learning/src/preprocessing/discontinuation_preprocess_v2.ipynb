{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "59517b77",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Raw data shape: (6612, 31)\n",
      "Columns: ['CASEID', 'AGE', 'AGE_GRP', 'REGION', 'EDUC_LEVEL', 'RELIGION', 'ETHNICITY', 'EDUC', 'HOUSEHOLD_HEAD_SEX', 'PARITY', 'CONTRACEPTIVE_METHOD', 'CURRENT_USE_TYPE', 'LAST_SOURCE_TYPE', 'MONTH_USE_CURRENT_METHOD', 'LAST_METHOD_DISCONTINUED', 'REASON_DISCONTINUED', 'PATTERN_USE', 'INTENTION_USE', 'CONTRACEPTIVE_USE_AND_INTENTION', 'WANT_LAST_CHILD', 'WANT_LAST_PREGNANCY', 'TOLD_ABT_SIDE_EFFECTS', 'SMOKE_CIGAR', 'MARITAL_STATUS', 'RESIDING_WITH_PARTNER', 'DESIRE_FOR_MORE_CHILDREN', 'HSBND_DESIRE_FOR_MORE_CHILDREN', 'OCCUPATION', 'HUSBANDS_EDUC', 'HUSBAND_AGE', 'PARTNER_EDUC']\n",
      "Filtered to current users. Shape: (3521, 31)\n",
      "\n",
      "Target value counts (including NaN):\n",
      "HIGH_RISK_DISCONTINUE\n",
      "0.0    2999\n",
      "NaN     316\n",
      "1.0     206\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Design 1 modeling sample shape: (3205, 32)\n",
      "Target distribution (%):\n",
      "HIGH_RISK_DISCONTINUE\n",
      "0    93.57\n",
      "1     6.43\n",
      "Name: proportion, dtype: float64\n",
      "\n",
      "Number of selected features: 25\n",
      "Selected feature columns:\n",
      "['AGE', 'REGION', 'EDUC_LEVEL', 'RELIGION', 'ETHNICITY', 'MARITAL_STATUS', 'RESIDING_WITH_PARTNER', 'HOUSEHOLD_HEAD_SEX', 'OCCUPATION', 'HUSBANDS_EDUC', 'HUSBAND_AGE', 'PARTNER_EDUC', 'SMOKE_CIGAR', 'PARITY', 'DESIRE_FOR_MORE_CHILDREN', 'WANT_LAST_CHILD', 'WANT_LAST_PREGNANCY', 'CONTRACEPTIVE_METHOD', 'MONTH_USE_CURRENT_METHOD', 'PATTERN_USE', 'TOLD_ABT_SIDE_EFFECTS', 'LAST_SOURCE_TYPE', 'LAST_METHOD_DISCONTINUED', 'REASON_DISCONTINUED', 'HSBND_DESIRE_FOR_MORE_CHILDREN']\n",
      "\n",
      "Final X shape: (3205, 25)\n",
      "Final y shape: (3205,)\n",
      "\n",
      "Train shape: (2564, 25) Test shape: (641, 25)\n",
      "Train target distribution (%):\n",
      "HIGH_RISK_DISCONTINUE\n",
      "0    93.56\n",
      "1     6.44\n",
      "Name: proportion, dtype: float64\n",
      "\n",
      "Numeric features: ['AGE', 'PARITY']\n",
      "Categorical features: ['REGION', 'EDUC_LEVEL', 'RELIGION', 'ETHNICITY', 'MARITAL_STATUS', 'RESIDING_WITH_PARTNER', 'HOUSEHOLD_HEAD_SEX', 'OCCUPATION', 'HUSBANDS_EDUC', 'HUSBAND_AGE', 'PARTNER_EDUC', 'SMOKE_CIGAR', 'DESIRE_FOR_MORE_CHILDREN', 'WANT_LAST_CHILD', 'WANT_LAST_PREGNANCY', 'CONTRACEPTIVE_METHOD', 'MONTH_USE_CURRENT_METHOD', 'PATTERN_USE', 'TOLD_ABT_SIDE_EFFECTS', 'LAST_SOURCE_TYPE', 'LAST_METHOD_DISCONTINUED', 'REASON_DISCONTINUED', 'HSBND_DESIRE_FOR_MORE_CHILDREN']\n",
      "\n",
      "=== Training XGBoost baseline ===\n",
      "\n",
      "=== XGBoost performance (HIGH_RISK_DISCONTINUE) ===\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0      0.941     0.985     0.963       600\n",
      "           1      0.308     0.098     0.148        41\n",
      "\n",
      "    accuracy                          0.928       641\n",
      "   macro avg      0.624     0.541     0.555       641\n",
      "weighted avg      0.901     0.928     0.910       641\n",
      "\n",
      "Confusion matrix (XGBoost):\n",
      "[[591   9]\n",
      " [ 37   4]]\n",
      "\n",
      "=== Training Decision Tree baseline ===\n",
      "\n",
      "=== Decision Tree performance (HIGH_RISK_DISCONTINUE) ===\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0      1.000     0.797     0.887       600\n",
      "           1      0.252     1.000     0.402        41\n",
      "\n",
      "    accuracy                          0.810       641\n",
      "   macro avg      0.626     0.898     0.644       641\n",
      "weighted avg      0.952     0.810     0.856       641\n",
      "\n",
      "Confusion matrix (Decision Tree):\n",
      "[[478 122]\n",
      " [  0  41]]\n",
      "\n",
      "=== Training hybrid components ===\n",
      "\n",
      "=== Hybrid (XGBoost + Decision Tree) performance ===\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0      0.944     0.958     0.951       600\n",
      "           1      0.219     0.171     0.192        41\n",
      "\n",
      "    accuracy                          0.908       641\n",
      "   macro avg      0.581     0.565     0.571       641\n",
      "weighted avg      0.898     0.908     0.903       641\n",
      "\n",
      "Confusion matrix (Hybrid):\n",
      "[[575  25]\n",
      " [ 34   7]]\n",
      "\n",
      "Fraction of test cases overridden by Decision Tree: 4.21%\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'\\nYou now have:\\n- df_model_d1: current users with intention-based HIGH_RISK_DISCONTINUE\\n- X_train, X_test, y_train, y_test\\n- xgb_pipeline, dt_pipeline, and a hybrid model\\n\\nNext steps:\\n- Inspect especially recall/precision for class 1 (high-risk).\\n- Tune XGBoost, Decision Tree, and CONF_MARGIN for your desired trade-off.\\n'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "Design 1 (updated, no leakage from history into label):\n",
    "- Population: CURRENT contraceptive users\n",
    "- Target: HIGH_RISK_DISCONTINUE, defined ONLY from intention variables\n",
    "- Features: demographics, fertility, method characteristics, history (as predictors only)\n",
    "\"\"\"\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "# -------------------------------------------------------------------\n",
    "# 1. Load data\n",
    "# -------------------------------------------------------------------\n",
    "\n",
    "DATA_PATH = \"../../data/interim/merged_dataset.csv\"  # adjust if needed\n",
    "df = pd.read_csv(DATA_PATH)\n",
    "\n",
    "print(\"Raw data shape:\", df.shape)\n",
    "print(\"Columns:\", list(df.columns))\n",
    "\n",
    "# -------------------------------------------------------------------\n",
    "# 2. Helper: safe membership check on mixed code/label columns\n",
    "# -------------------------------------------------------------------\n",
    "\n",
    "def is_in(series: pd.Series, values) -> pd.Series:\n",
    "    \"\"\"\n",
    "    Robust membership check that works if the column has mixed types\n",
    "    (e.g. numeric codes and string labels).\n",
    "    \"\"\"\n",
    "    values_str = set(str(v) for v in values)\n",
    "    return series.astype(str).isin(values_str)\n",
    "\n",
    "# -------------------------------------------------------------------\n",
    "# 3. Filter to CURRENT users only\n",
    "#    Treat '3' and 'Current user' as current user status.\n",
    "# -------------------------------------------------------------------\n",
    "\n",
    "if \"CURRENT_USE_TYPE\" not in df.columns:\n",
    "    raise ValueError(\"Column 'CURRENT_USE_TYPE' not found in the dataset.\")\n",
    "\n",
    "current_user_values = [\"3\", \"Current user\"]\n",
    "df_current = df[is_in(df[\"CURRENT_USE_TYPE\"], current_user_values)].copy()\n",
    "\n",
    "print(\"Filtered to current users. Shape:\", df_current.shape)\n",
    "\n",
    "# -------------------------------------------------------------------\n",
    "# 4. Construct HIGH_RISK_DISCONTINUE target (intention-based only)\n",
    "#    Logic:\n",
    "#    - High risk (1) if intention suggests \"using but intends to stop\" or similar\n",
    "#    - Low risk (0) if intention suggests \"using and intends to continue\"\n",
    "#    - Others => NaN (dropped later)\n",
    "# -------------------------------------------------------------------\n",
    "\n",
    "# 4.1 Intention-based risk using CONTRACEPTIVE_USE_AND_INTENTION\n",
    "if \"CONTRACEPTIVE_USE_AND_INTENTION\" in df_current.columns:\n",
    "    high_risk_intention_values = [\n",
    "        \"3\",                           # numeric code (example) for \"Using but intends to stop\"\n",
    "        \"Using but intends to stop\",\n",
    "        \"Using but unsure\",\n",
    "    ]\n",
    "    low_risk_intention_values = [\n",
    "        \"1\",                           # numeric code (example) for \"Using and intends to continue\"\n",
    "        \"Using and intends to continue\",\n",
    "    ]\n",
    "    intention_high_risk = is_in(df_current[\"CONTRACEPTIVE_USE_AND_INTENTION\"],\n",
    "                                high_risk_intention_values)\n",
    "    intention_low_risk = is_in(df_current[\"CONTRACEPTIVE_USE_AND_INTENTION\"],\n",
    "                               low_risk_intention_values)\n",
    "else:\n",
    "    intention_high_risk = pd.Series(False, index=df_current.index)\n",
    "    intention_low_risk = pd.Series(False, index=df_current.index)\n",
    "\n",
    "# 4.2 Additional INTENTION_USE-based risk (secondary hint)\n",
    "if \"INTENTION_USE\" in df_current.columns:\n",
    "    high_risk_INTENTION_USE_values = [\n",
    "        \"4\", \"5\", \"7\",   # example codes: \"no intention\"/\"undecided\"/\"intends to stop\"\n",
    "        \"No intention\",\n",
    "        \"Intends to stop\",\n",
    "        \"Undecided\",\n",
    "    ]\n",
    "    low_risk_INTENTION_USE_values = [\n",
    "        \"1\", \"2\", \"3\",\n",
    "        \"Intends to continue\",\n",
    "        \"Intends to use\",\n",
    "    ]\n",
    "    intention_use_high_risk = is_in(df_current[\"INTENTION_USE\"],\n",
    "                                    high_risk_INTENTION_USE_values)\n",
    "    intention_use_low_risk = is_in(df_current[\"INTENTION_USE\"],\n",
    "                                   low_risk_INTENTION_USE_values)\n",
    "else:\n",
    "    intention_use_high_risk = pd.Series(False, index=df_current.index)\n",
    "    intention_use_low_risk = pd.Series(False, index=df_current.index)\n",
    "\n",
    "# Combine intention signals (ONLY intentions, no history in label)\n",
    "any_intention_high_risk = intention_high_risk | intention_use_high_risk\n",
    "any_intention_low_risk = intention_low_risk | intention_use_low_risk\n",
    "\n",
    "# 4.3 Final target: HIGH_RISK_DISCONTINUE (intention-based)\n",
    "target = np.where(\n",
    "    any_intention_high_risk,\n",
    "    1,\n",
    "    np.where(\n",
    "        any_intention_low_risk & ~any_intention_high_risk,\n",
    "        0,\n",
    "        np.nan\n",
    "    )\n",
    ")\n",
    "\n",
    "df_current[\"HIGH_RISK_DISCONTINUE\"] = target\n",
    "\n",
    "print(\"\\nTarget value counts (including NaN):\")\n",
    "print(df_current[\"HIGH_RISK_DISCONTINUE\"].value_counts(dropna=False))\n",
    "\n",
    "# Keep only rows with defined target\n",
    "df_model_d1 = df_current.dropna(subset=[\"HIGH_RISK_DISCONTINUE\"]).copy()\n",
    "df_model_d1[\"HIGH_RISK_DISCONTINUE\"] = df_model_d1[\"HIGH_RISK_DISCONTINUE\"].astype(int)\n",
    "\n",
    "print(\"\\nDesign 1 modeling sample shape:\", df_model_d1.shape)\n",
    "print(\"Target distribution (%):\")\n",
    "print((df_model_d1[\"HIGH_RISK_DISCONTINUE\"].value_counts(normalize=True) * 100).round(2))\n",
    "\n",
    "# -------------------------------------------------------------------\n",
    "# 5. Define feature set (no intention columns to avoid label leakage)\n",
    "#    We allow history variables (LAST_METHOD_DISCONTINUED, REASON_DISCONTINUED)\n",
    "#    as predictors, but they are NOT used in the label definition.\n",
    "# -------------------------------------------------------------------\n",
    "\n",
    "feature_cols_demo = [\n",
    "    \"AGE\", \"REGION\", \"EDUC_LEVEL\", \"RELIGION\", \"ETHNICITY\",\n",
    "    \"MARITAL_STATUS\", \"RESIDING_WITH_PARTNER\",\n",
    "    \"HOUSEHOLD_HEAD_SEX\", \"OCCUPATION\",\n",
    "    \"HUSBANDS_EDUC\", \"HUSBAND_AGE\", \"PARTNER_EDUC\",\n",
    "    \"SMOKE_CIGAR\"\n",
    "]\n",
    "\n",
    "feature_cols_fertility = [\n",
    "    \"PARITY\",\n",
    "    \"DESIRE_FOR_MORE_CHILDREN\",\n",
    "    \"WANT_LAST_CHILD\",\n",
    "    \"WANT_LAST_PREGNANCY\",\n",
    "]\n",
    "\n",
    "feature_cols_method = [\n",
    "    \"CONTRACEPTIVE_METHOD\",\n",
    "    \"MONTH_USE_CURRENT_METHOD\",\n",
    "    \"PATTERN_USE\",\n",
    "    \"TOLD_ABT_SIDE_EFFECTS\",\n",
    "    \"LAST_SOURCE_TYPE\",\n",
    "    \"LAST_METHOD_DISCONTINUED\",      # history as feature (OK now)\n",
    "    \"REASON_DISCONTINUED\",           # history as feature (OK now)\n",
    "    \"HSBND_DESIRE_FOR_MORE_CHILDREN\",\n",
    "]\n",
    "\n",
    "# Columns to exclude from features (direct definition of target)\n",
    "leakage_cols = [\n",
    "    \"CONTRACEPTIVE_USE_AND_INTENTION\",\n",
    "    \"INTENTION_USE\",\n",
    "    \"HIGH_RISK_DISCONTINUE\",\n",
    "]\n",
    "\n",
    "all_candidate_features = feature_cols_demo + feature_cols_fertility + feature_cols_method\n",
    "feature_cols = [\n",
    "    c for c in all_candidate_features\n",
    "    if c in df_model_d1.columns and c not in leakage_cols\n",
    "]\n",
    "\n",
    "print(\"\\nNumber of selected features:\", len(feature_cols))\n",
    "print(\"Selected feature columns:\")\n",
    "print(feature_cols)\n",
    "\n",
    "TARGET_D1 = \"HIGH_RISK_DISCONTINUE\"\n",
    "X = df_model_d1[feature_cols].copy()\n",
    "y = df_model_d1[TARGET_D1].copy()\n",
    "\n",
    "print(\"\\nFinal X shape:\", X.shape)\n",
    "print(\"Final y shape:\", y.shape)\n",
    "\n",
    "# -------------------------------------------------------------------\n",
    "# 6. Train-test split (stratified)\n",
    "# -------------------------------------------------------------------\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X,\n",
    "    y,\n",
    "    test_size=0.2,\n",
    "    random_state=42,\n",
    "    stratify=y\n",
    ")\n",
    "\n",
    "print(\"\\nTrain shape:\", X_train.shape, \"Test shape:\", X_test.shape)\n",
    "print(\"Train target distribution (%):\")\n",
    "print((y_train.value_counts(normalize=True) * 100).round(2))\n",
    "\n",
    "# -------------------------------------------------------------------\n",
    "# 7. Identify numeric vs categorical features\n",
    "# -------------------------------------------------------------------\n",
    "\n",
    "all_features = list(X_train.columns)\n",
    "numeric_features = [\n",
    "    col for col in all_features\n",
    "    if pd.api.types.is_numeric_dtype(X_train[col])\n",
    "]\n",
    "categorical_features = [\n",
    "    col for col in all_features\n",
    "    if col not in numeric_features\n",
    "]\n",
    "\n",
    "print(\"\\nNumeric features:\", numeric_features)\n",
    "print(\"Categorical features:\", categorical_features)\n",
    "\n",
    "# -------------------------------------------------------------------\n",
    "# 8. Preprocessing: imputation + one-hot encoding\n",
    "# -------------------------------------------------------------------\n",
    "\n",
    "numeric_transformer = Pipeline(steps=[\n",
    "    (\"imputer\", SimpleImputer(strategy=\"median\")),\n",
    "])\n",
    "\n",
    "categorical_transformer = Pipeline(steps=[\n",
    "    (\"imputer\", SimpleImputer(strategy=\"most_frequent\")),\n",
    "    (\"onehot\", OneHotEncoder(handle_unknown=\"ignore\")),\n",
    "])\n",
    "\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        (\"num\", numeric_transformer, numeric_features),\n",
    "        (\"cat\", categorical_transformer, categorical_features),\n",
    "    ]\n",
    ")\n",
    "\n",
    "# -------------------------------------------------------------------\n",
    "# 9. Train XGBoost baseline\n",
    "# -------------------------------------------------------------------\n",
    "\n",
    "xgb_clf = XGBClassifier(\n",
    "    n_estimators=300,\n",
    "    max_depth=5,\n",
    "    learning_rate=0.05,\n",
    "    subsample=0.8,\n",
    "    colsample_bytree=0.8,\n",
    "    objective=\"binary:logistic\",\n",
    "    tree_method=\"hist\",\n",
    "    eval_metric=\"logloss\",\n",
    "    random_state=42,\n",
    ")\n",
    "\n",
    "xgb_pipeline = Pipeline(steps=[\n",
    "    (\"preprocess\", preprocessor),\n",
    "    (\"model\", xgb_clf)\n",
    "])\n",
    "\n",
    "print(\"\\n=== Training XGBoost baseline ===\")\n",
    "xgb_pipeline.fit(X_train, y_train)\n",
    "y_pred_xgb = xgb_pipeline.predict(X_test)\n",
    "\n",
    "print(\"\\n=== XGBoost performance (HIGH_RISK_DISCONTINUE) ===\")\n",
    "print(classification_report(y_test, y_pred_xgb, digits=3))\n",
    "print(\"Confusion matrix (XGBoost):\")\n",
    "print(confusion_matrix(y_test, y_pred_xgb))\n",
    "\n",
    "# -------------------------------------------------------------------\n",
    "# 10. Train Decision Tree baseline\n",
    "# -------------------------------------------------------------------\n",
    "\n",
    "dt_clf = DecisionTreeClassifier(\n",
    "    max_depth=5,\n",
    "    min_samples_leaf=50,\n",
    "    random_state=42,\n",
    "    class_weight=\"balanced\"\n",
    ")\n",
    "\n",
    "dt_pipeline = Pipeline(steps=[\n",
    "    (\"preprocess\", preprocessor),\n",
    "    (\"model\", dt_clf)\n",
    "])\n",
    "\n",
    "print(\"\\n=== Training Decision Tree baseline ===\")\n",
    "dt_pipeline.fit(X_train, y_train)\n",
    "y_pred_dt = dt_pipeline.predict(X_test)\n",
    "\n",
    "print(\"\\n=== Decision Tree performance (HIGH_RISK_DISCONTINUE) ===\")\n",
    "print(classification_report(y_test, y_pred_dt, digits=3))\n",
    "print(\"Confusion matrix (Decision Tree):\")\n",
    "print(confusion_matrix(y_test, y_pred_dt))\n",
    "\n",
    "# -------------------------------------------------------------------\n",
    "# 11. Hybrid model: XGBoost + Decision Tree override on low-confidence cases\n",
    "# -------------------------------------------------------------------\n",
    "\n",
    "print(\"\\n=== Training hybrid components ===\")\n",
    "\n",
    "xgb_hybrid = XGBClassifier(\n",
    "    n_estimators=300,\n",
    "    max_depth=5,\n",
    "    learning_rate=0.05,\n",
    "    subsample=0.8,\n",
    "    colsample_bytree=0.8,\n",
    "    objective=\"binary:logistic\",\n",
    "    tree_method=\"hist\",\n",
    "    eval_metric=\"logloss\",\n",
    "    random_state=42,\n",
    ")\n",
    "\n",
    "xgb_pipeline_hybrid = Pipeline(steps=[\n",
    "    (\"preprocess\", preprocessor),\n",
    "    (\"model\", xgb_hybrid)\n",
    "])\n",
    "\n",
    "dt_hybrid = DecisionTreeClassifier(\n",
    "    max_depth=5,\n",
    "    min_samples_leaf=50,\n",
    "    random_state=42,\n",
    "    class_weight=\"balanced\"\n",
    ")\n",
    "\n",
    "dt_pipeline_hybrid = Pipeline(steps=[\n",
    "    (\"preprocess\", preprocessor),\n",
    "    (\"model\", dt_hybrid)\n",
    "])\n",
    "\n",
    "xgb_pipeline_hybrid.fit(X_train, y_train)\n",
    "dt_pipeline_hybrid.fit(X_train, y_train)\n",
    "\n",
    "# Probabilities from XGBoost\n",
    "proba_xgb = xgb_pipeline_hybrid.predict_proba(X_test)[:, 1]  # P(y=1)\n",
    "pred_xgb = (proba_xgb >= 0.5).astype(int)\n",
    "\n",
    "# Predictions from Decision Tree\n",
    "pred_dt = dt_pipeline_hybrid.predict(X_test)\n",
    "\n",
    "# Confidence margin for hybrid switching\n",
    "CONF_MARGIN = 0.1  # tune this: smaller = fewer overrides\n",
    "confidence_xgb = np.abs(proba_xgb - 0.5)\n",
    "use_dt = confidence_xgb < CONF_MARGIN\n",
    "\n",
    "hybrid_pred = np.where(use_dt, pred_dt, pred_xgb)\n",
    "\n",
    "print(\"\\n=== Hybrid (XGBoost + Decision Tree) performance ===\")\n",
    "print(classification_report(y_test, hybrid_pred, digits=3))\n",
    "print(\"Confusion matrix (Hybrid):\")\n",
    "print(confusion_matrix(y_test, hybrid_pred))\n",
    "\n",
    "override_rate = use_dt.mean() * 100\n",
    "print(f\"\\nFraction of test cases overridden by Decision Tree: {override_rate:.2f}%\")\n",
    "\n",
    "\"\"\"\n",
    "You now have:\n",
    "- df_model_d1: current users with intention-based HIGH_RISK_DISCONTINUE\n",
    "- X_train, X_test, y_train, y_test\n",
    "- xgb_pipeline, dt_pipeline, and a hybrid model\n",
    "\n",
    "Next steps:\n",
    "- Inspect especially recall/precision for class 1 (high-risk).\n",
    "- Tune XGBoost, Decision Tree, and CONF_MARGIN for your desired trade-off.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "58386318",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['../models/discontinuation_design1_dt_model_d1_v2.pkl']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import joblib\n",
    "joblib.dump((X_train, X_test, y_train, y_test), '../../data/processed/discontinuation_design1_data_v2.pkl')\n",
    "joblib.dump(df_model_d1, '../../data/processed/discontinuation_design1_full_data_v2.pkl')\n",
    "joblib.dump(xgb_pipeline_hybrid, '../models/discontinuation_design1_xgb_hybrid_model_d1_v2.pkl')\n",
    "joblib.dump(dt_pipeline_hybrid, '../models/discontinuation_design1_dt_hybrid_model_d1_v2.pkl')\n",
    "joblib.dump(xgb_pipeline, '../models/discontinuation_design1_xgb_model_d1_v2.pkl')\n",
    "joblib.dump(dt_pipeline, '../models/discontinuation_design1_dt_model_d1_v2.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d3709b60",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Baseline scale_pos_weight: 14.539393939393939\n",
      "Boosted scale_pos_weight (v2): 21.80909090909091\n",
      "\n",
      "=== Training XGBoost v2 (weighted) ===\n",
      "\n",
      "=== XGBoost v2 performance (baseline threshold 0.5) ===\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0      0.969     0.887     0.926       600\n",
      "           1      0.261     0.585     0.361        41\n",
      "\n",
      "    accuracy                          0.867       641\n",
      "   macro avg      0.615     0.736     0.643       641\n",
      "weighted avg      0.924     0.867     0.890       641\n",
      "\n",
      "Confusion matrix (XGBoost v2 @0.5):\n",
      "[[532  68]\n",
      " [ 17  24]]\n",
      "\n",
      "=== Training Decision Tree v2 (weighted) ===\n",
      "\n",
      "=== Decision Tree v2 performance (default threshold) ===\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0      0.973     0.892     0.930       600\n",
      "           1      0.286     0.634     0.394        41\n",
      "\n",
      "    accuracy                          0.875       641\n",
      "   macro avg      0.629     0.763     0.662       641\n",
      "weighted avg      0.929     0.875     0.896       641\n",
      "\n",
      "Confusion matrix (Decision Tree v2):\n",
      "[[535  65]\n",
      " [ 15  26]]\n",
      "\n",
      "=== Threshold scan for XGBoost v2 ===\n",
      "\n",
      "=== XGBoost v2 @ THRESH=0.50 ===\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0      0.969     0.887     0.926       600\n",
      "           1      0.261     0.585     0.361        41\n",
      "\n",
      "    accuracy                          0.867       641\n",
      "   macro avg      0.615     0.736     0.643       641\n",
      "weighted avg      0.924     0.867     0.890       641\n",
      "\n",
      "\n",
      "=== XGBoost v2 @ THRESH=0.40 ===\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0      0.974     0.867     0.917       600\n",
      "           1      0.252     0.659     0.365        41\n",
      "\n",
      "    accuracy                          0.853       641\n",
      "   macro avg      0.613     0.763     0.641       641\n",
      "weighted avg      0.928     0.853     0.882       641\n",
      "\n",
      "\n",
      "=== XGBoost v2 @ THRESH=0.30 ===\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0      0.977     0.847     0.907       600\n",
      "           1      0.240     0.707     0.358        41\n",
      "\n",
      "    accuracy                          0.838       641\n",
      "   macro avg      0.608     0.777     0.633       641\n",
      "weighted avg      0.930     0.838     0.872       641\n",
      "\n",
      "\n",
      "=== XGBoost v2 @ THRESH=0.25 ===\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0      0.981     0.838     0.904       600\n",
      "           1      0.242     0.756     0.367        41\n",
      "\n",
      "    accuracy                          0.833       641\n",
      "   macro avg      0.611     0.797     0.635       641\n",
      "weighted avg      0.933     0.833     0.870       641\n",
      "\n",
      "\n",
      "=== XGBoost v2 @ THRESH=0.20 ===\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0      0.984     0.832     0.902       600\n",
      "           1      0.246     0.805     0.377        41\n",
      "\n",
      "    accuracy                          0.830       641\n",
      "   macro avg      0.615     0.818     0.639       641\n",
      "weighted avg      0.937     0.830     0.868       641\n",
      "\n",
      "\n",
      "=== XGBoost v2 @ THRESH=0.15 ===\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0      0.988     0.825     0.899       600\n",
      "           1      0.250     0.854     0.387        41\n",
      "\n",
      "    accuracy                          0.827       641\n",
      "   macro avg      0.619     0.839     0.643       641\n",
      "weighted avg      0.941     0.827     0.866       641\n",
      "\n",
      "\n",
      "=== Hybrid v2 (XGBoost v2 + Decision Tree v2, upgrade-only) ===\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0      0.984     0.827     0.899       600\n",
      "           1      0.241     0.805     0.371        41\n",
      "\n",
      "    accuracy                          0.825       641\n",
      "   macro avg      0.613     0.816     0.635       641\n",
      "weighted avg      0.937     0.825     0.865       641\n",
      "\n",
      "Confusion matrix (Hybrid v2):\n",
      "[[496 104]\n",
      " [  8  33]]\n",
      "\n",
      "Fraction of test cases where DT v2 can upgrade to 1: 6.40%\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'\\nInterpretation notes (for documentation):\\n\\n- XGBoost v2 uses boosted scale_pos_weight_v2 to penalize errors on class 1 more.\\n- Decision Tree v2 uses class_weight={0:1, 1:3} and a deeper tree to better capture\\n  high-risk patterns.\\n- The threshold scan shows how recall and precision on class 1 change as we lower THRESH.\\n- Hybrid v2 starts from the low-threshold XGBoost v2 predictions and only allows\\n  the Decision Tree v2 to \"upgrade\" uncertain cases to high-risk (class 1) in a\\n  low-confidence band around THRESH_V2.\\n\\nTo target a specific recall (e.g., ~0.85), adjust:\\n- scale_pos_weight_v2 (e.g., try 2x instead of 1.5x),\\n- THRESH_V2 (e.g., 0.20 → 0.15),\\n- CONF_MARGIN_V2 (e.g., 0.15 → 0.20),\\nand re-run this block to compare the resulting classification_report\\nfor class 1 (high-risk).\\n'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "Model v2: High-recall variant for HIGH_RISK_DISCONTINUE\n",
    "\n",
    "Requirements:\n",
    "- Assumes X_train, X_test, y_train, y_test, preprocessor, numeric_features, categorical_features\n",
    "  are already defined from the earlier code.\n",
    "- Does NOT change any earlier variables or models; it creates new ones with suffix `_v2`.\n",
    "\n",
    "Goals:\n",
    "- Increase recall for the high-risk class (1), targeting ~0.85 recall.\n",
    "- Use:\n",
    "    * class weighting (scale_pos_weight) for XGBoost\n",
    "    * class_weight for Decision Tree\n",
    "    * lower decision threshold for XGBoost\n",
    "    * \"upgrade-only\" hybrid override by Decision Tree\n",
    "\"\"\"\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "# -------------------------------------------------------------------\n",
    "# 1. Compute class imbalance and boosted scale_pos_weight\n",
    "# -------------------------------------------------------------------\n",
    "\n",
    "pos_v2 = (y_train == 1).sum()\n",
    "neg_v2 = (y_train == 0).sum()\n",
    "scale_pos_weight_v2 = neg_v2 / max(pos_v2, 1)\n",
    "\n",
    "print(\"Baseline scale_pos_weight:\", scale_pos_weight_v2)\n",
    "\n",
    "# Boost it a bit to further emphasize class 1\n",
    "scale_pos_weight_v2 *= 1.5\n",
    "print(\"Boosted scale_pos_weight (v2):\", scale_pos_weight_v2)\n",
    "\n",
    "# -------------------------------------------------------------------\n",
    "# 2. Define v2 models (weighted XGBoost + stronger DT for class 1)\n",
    "# -------------------------------------------------------------------\n",
    "\n",
    "# XGBoost v2 (weighted)\n",
    "xgb_clf_v2 = XGBClassifier(\n",
    "    n_estimators=300,\n",
    "    max_depth=5,\n",
    "    learning_rate=0.05,\n",
    "    subsample=0.8,\n",
    "    colsample_bytree=0.8,\n",
    "    objective=\"binary:logistic\",\n",
    "    tree_method=\"hist\",\n",
    "    eval_metric=\"logloss\",\n",
    "    random_state=42,\n",
    "    scale_pos_weight=scale_pos_weight_v2  # emphasize class 1\n",
    ")\n",
    "\n",
    "xgb_pipeline_v2 = Pipeline(steps=[\n",
    "    (\"preprocess\", preprocessor),\n",
    "    (\"model\", xgb_clf_v2)\n",
    "])\n",
    "\n",
    "# Decision Tree v2 (more depth + class weight)\n",
    "dt_clf_v2 = DecisionTreeClassifier(\n",
    "    max_depth=6,             # slightly deeper than v1\n",
    "    min_samples_leaf=20,     # smaller leaves, more granularity\n",
    "    random_state=42,\n",
    "    class_weight={0: 1.0, 1: 3.0}  # emphasize class 1\n",
    ")\n",
    "\n",
    "dt_pipeline_v2 = Pipeline(steps=[\n",
    "    (\"preprocess\", preprocessor),\n",
    "    (\"model\", dt_clf_v2)\n",
    "])\n",
    "\n",
    "# -------------------------------------------------------------------\n",
    "# 3. Fit v2 models\n",
    "# -------------------------------------------------------------------\n",
    "\n",
    "print(\"\\n=== Training XGBoost v2 (weighted) ===\")\n",
    "xgb_pipeline_v2.fit(X_train, y_train)\n",
    "y_pred_xgb_v2 = xgb_pipeline_v2.predict(X_test)\n",
    "\n",
    "print(\"\\n=== XGBoost v2 performance (baseline threshold 0.5) ===\")\n",
    "print(classification_report(y_test, y_pred_xgb_v2, digits=3))\n",
    "print(\"Confusion matrix (XGBoost v2 @0.5):\")\n",
    "print(confusion_matrix(y_test, y_pred_xgb_v2))\n",
    "\n",
    "print(\"\\n=== Training Decision Tree v2 (weighted) ===\")\n",
    "dt_pipeline_v2.fit(X_train, y_train)\n",
    "y_pred_dt_v2 = dt_pipeline_v2.predict(X_test)\n",
    "\n",
    "print(\"\\n=== Decision Tree v2 performance (default threshold) ===\")\n",
    "print(classification_report(y_test, y_pred_dt_v2, digits=3))\n",
    "print(\"Confusion matrix (Decision Tree v2):\")\n",
    "print(confusion_matrix(y_test, y_pred_dt_v2))\n",
    "\n",
    "# -------------------------------------------------------------------\n",
    "# 4. Threshold scan for XGBoost v2 (no hybrid)\n",
    "#    to understand recall/precision trade-off on class 1\n",
    "# -------------------------------------------------------------------\n",
    "\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "def evaluate_threshold_xgb_v2(thresh):\n",
    "    proba = xgb_pipeline_v2.predict_proba(X_test)[:, 1]\n",
    "    pred = (proba >= thresh).astype(int)\n",
    "    print(f\"\\n=== XGBoost v2 @ THRESH={thresh:.2f} ===\")\n",
    "    print(classification_report(y_test, pred, digits=3))\n",
    "\n",
    "print(\"\\n=== Threshold scan for XGBoost v2 ===\")\n",
    "for t in [0.50, 0.40, 0.30, 0.25, 0.20, 0.15]:\n",
    "    evaluate_threshold_xgb_v2(t)\n",
    "\n",
    "# -------------------------------------------------------------------\n",
    "# 5. High-recall Hybrid v2: upgrade-only rule\n",
    "#    Strategy:\n",
    "#      - Use a lower threshold for XGBoost v2 (e.g., 0.20)\n",
    "#      - Define a low-confidence zone around that threshold\n",
    "#      - If in low-confidence zone AND DT says 1, upgrade to 1\n",
    "# -------------------------------------------------------------------\n",
    "\n",
    "# Choose a threshold for high recall (adjust after looking at the scan above)\n",
    "THRESH_V2 = 0.20   # starting point; consider 0.15 if recall still too low\n",
    "CONF_MARGIN_V2 = 0.15  # low-confidence band around THRESH_V2\n",
    "\n",
    "# Probabilities from XGBoost v2\n",
    "proba_xgb_v2 = xgb_pipeline_v2.predict_proba(X_test)[:, 1]  # P(y=1)\n",
    "\n",
    "# Base XGBoost v2 prediction with lowered threshold\n",
    "pred_xgb_v2 = (proba_xgb_v2 >= THRESH_V2).astype(int)\n",
    "\n",
    "# Decision Tree v2 prediction\n",
    "pred_dt_v2 = dt_pipeline_v2.predict(X_test)\n",
    "\n",
    "# Confidence relative to THRESH_V2\n",
    "confidence_xgb_v2 = np.abs(proba_xgb_v2 - THRESH_V2)\n",
    "use_dt_v2 = confidence_xgb_v2 < CONF_MARGIN_V2\n",
    "\n",
    "# UPGRADE-ONLY HYBRID:\n",
    "#   Start from XGBoost v2 prediction.\n",
    "#   If XGBoost is low-confidence AND DT predicts 1, set final prediction to 1.\n",
    "hybrid_pred_v2 = pred_xgb_v2.copy()\n",
    "hybrid_pred_v2[(use_dt_v2) & (pred_dt_v2 == 1)] = 1\n",
    "\n",
    "print(\"\\n=== Hybrid v2 (XGBoost v2 + Decision Tree v2, upgrade-only) ===\")\n",
    "print(classification_report(y_test, hybrid_pred_v2, digits=3))\n",
    "print(\"Confusion matrix (Hybrid v2):\")\n",
    "print(confusion_matrix(y_test, hybrid_pred_v2))\n",
    "\n",
    "override_rate_v2 = use_dt_v2.mean() * 100\n",
    "print(f\"\\nFraction of test cases where DT v2 can upgrade to 1: {override_rate_v2:.2f}%\")\n",
    "\n",
    "\"\"\"\n",
    "Interpretation notes (for documentation):\n",
    "\n",
    "- XGBoost v2 uses boosted scale_pos_weight_v2 to penalize errors on class 1 more.\n",
    "- Decision Tree v2 uses class_weight={0:1, 1:3} and a deeper tree to better capture\n",
    "  high-risk patterns.\n",
    "- The threshold scan shows how recall and precision on class 1 change as we lower THRESH.\n",
    "- Hybrid v2 starts from the low-threshold XGBoost v2 predictions and only allows\n",
    "  the Decision Tree v2 to \"upgrade\" uncertain cases to high-risk (class 1) in a\n",
    "  low-confidence band around THRESH_V2.\n",
    "\n",
    "To target a specific recall (e.g., ~0.85), adjust:\n",
    "- scale_pos_weight_v2 (e.g., try 2x instead of 1.5x),\n",
    "- THRESH_V2 (e.g., 0.20 → 0.15),\n",
    "- CONF_MARGIN_V2 (e.g., 0.15 → 0.20),\n",
    "and re-run this block to compare the resulting classification_report\n",
    "for class 1 (high-risk).\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f2f6f2ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Hybrid v3 (more aggressive high-recall) ===\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0      0.990     0.818     0.896       600\n",
      "           1      0.248     0.878     0.387        41\n",
      "\n",
      "    accuracy                          0.822       641\n",
      "   macro avg      0.619     0.848     0.642       641\n",
      "weighted avg      0.942     0.822     0.863       641\n",
      "\n",
      "Confusion matrix (Hybrid v3):\n",
      "[[491 109]\n",
      " [  5  36]]\n",
      "\n",
      "Fraction of test cases where DT v2 can upgrade to 1 (v3): 82.37%\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Model v3: Slightly more aggressive high-recall variant\n",
    "\n",
    "Goal: Increase recall for class 1 a bit beyond Hybrid v2 (~0.805),\n",
    "      by lowering THRESH further and widening CONF_MARGIN slightly.\n",
    "Assumes:\n",
    "- xgb_pipeline_v2, dt_pipeline_v2, X_test, y_test already defined and fitted (from v2).\n",
    "\"\"\"\n",
    "\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "import numpy as np\n",
    "\n",
    "# More aggressive threshold towards class 1\n",
    "THRESH_V3 = 0.15       # was 0.20 in v2\n",
    "CONF_MARGIN_V3 = 0.20  # was 0.15 in v2\n",
    "\n",
    "# Probabilities from XGBoost v2\n",
    "proba_xgb_v3 = xgb_pipeline_v2.predict_proba(X_test)[:, 1]  # P(y=1)\n",
    "\n",
    "# Base XGBoost v2 prediction with lower threshold\n",
    "pred_xgb_v3 = (proba_xgb_v3 >= THRESH_V3).astype(int)\n",
    "\n",
    "# Decision Tree v2 prediction (same model as in v2)\n",
    "pred_dt_v3 = dt_pipeline_v2.predict(X_test)\n",
    "\n",
    "# Confidence relative to THRESH_V3\n",
    "confidence_xgb_v3 = np.abs(proba_xgb_v3 - THRESH_V3)\n",
    "use_dt_v3 = confidence_xgb_v3 < CONF_MARGIN_V3\n",
    "\n",
    "# UPGRADE-ONLY HYBRID v3:\n",
    "# Start from XGBoost v2 prediction.\n",
    "# If XGBoost is low-confidence AND DT predicts 1, set final prediction to 1.\n",
    "hybrid_pred_v3 = pred_xgb_v3.copy()\n",
    "hybrid_pred_v3[(use_dt_v3) & (pred_dt_v3 == 1)] = 1\n",
    "\n",
    "print(\"\\n=== Hybrid v3 (more aggressive high-recall) ===\")\n",
    "print(classification_report(y_test, hybrid_pred_v3, digits=3))\n",
    "print(\"Confusion matrix (Hybrid v3):\")\n",
    "print(confusion_matrix(y_test, hybrid_pred_v3))\n",
    "\n",
    "override_rate_v3 = use_dt_v3.mean() * 100\n",
    "print(f\"\\nFraction of test cases where DT v2 can upgrade to 1 (v3): {override_rate_v3:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "29e34810",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved XGBoost high-recall pipeline to: models_high_risk_v3\\xgb_high_recall.joblib\n",
      "Saved Decision Tree high-recall pipeline to: models_high_risk_v3\\dt_high_recall.joblib\n",
      "Saved Hybrid v3 config to: models_high_risk_v3\\hybrid_v3_config.json\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Export block for high-recall pipelines:\n",
    "- Renames v2 pipelines to xgb_high_recall and dt_high_recall (used in Hybrid v3).\n",
    "- Saves them plus a Hybrid v3 config file.\n",
    "Assumes:\n",
    "- xgb_pipeline_v2, dt_pipeline_v2, scale_pos_weight_v2, THRESH_V3, CONF_MARGIN_V3\n",
    "  are already defined and fitted.\n",
    "\"\"\"\n",
    "\n",
    "import joblib\n",
    "import json\n",
    "import os\n",
    "\n",
    "# -------------------------------------------------------------------\n",
    "# 1. Alias pipelines with high-recall names\n",
    "# -------------------------------------------------------------------\n",
    "\n",
    "xgb_high_recall = xgb_pipeline_v2\n",
    "dt_high_recall = dt_pipeline_v2\n",
    "\n",
    "# -------------------------------------------------------------------\n",
    "# 2. Define output directory\n",
    "# -------------------------------------------------------------------\n",
    "\n",
    "MODEL_DIR = \"models_high_risk_v3\"\n",
    "os.makedirs(MODEL_DIR, exist_ok=True)\n",
    "\n",
    "# -------------------------------------------------------------------\n",
    "# 3. Save the fitted high-recall pipelines\n",
    "# -------------------------------------------------------------------\n",
    "\n",
    "xgb_path = os.path.join(MODEL_DIR, \"xgb_high_recall.joblib\")\n",
    "dt_path = os.path.join(MODEL_DIR, \"dt_high_recall.joblib\")\n",
    "\n",
    "joblib.dump(xgb_high_recall, xgb_path)\n",
    "joblib.dump(dt_high_recall, dt_path)\n",
    "\n",
    "print(f\"Saved XGBoost high-recall pipeline to: {xgb_path}\")\n",
    "print(f\"Saved Decision Tree high-recall pipeline to: {dt_path}\")\n",
    "\n",
    "# -------------------------------------------------------------------\n",
    "# 4. Save Hybrid v3 configuration (thresholds, weights, names)\n",
    "# -------------------------------------------------------------------\n",
    "\n",
    "hybrid_v3_config = {\n",
    "    \"description\": \"Hybrid v3: high-recall configuration for HIGH_RISK_DISCONTINUE\",\n",
    "    \"xgb_model_file\": \"xgb_high_recall.joblib\",\n",
    "    \"dt_model_file\": \"dt_high_recall.joblib\",\n",
    "    \"scale_pos_weight_v2\": float(scale_pos_weight_v2),\n",
    "    \"threshold_v3\": float(THRESH_V3),          # e.g. 0.15\n",
    "    \"conf_margin_v3\": float(CONF_MARGIN_V3),   # e.g. 0.20\n",
    "    \"hybrid_rule\": \"upgrade_only_if_low_confidence_and_dt_predicts_1\",\n",
    "    \"target_name\": \"HIGH_RISK_DISCONTINUE\",\n",
    "    \"notes\": (\n",
    "        \"xgb_high_recall: XGBoost v2 trained with boosted scale_pos_weight_v2. \"\n",
    "        \"dt_high_recall: DecisionTree v2 trained with class_weight={0:1.0, 1:3.0}. \"\n",
    "        \"Inference steps: \"\n",
    "        \"1) Use xgb_high_recall to get P(y=1). \"\n",
    "        \"2) Base prediction = 1 if P>=threshold_v3 else 0. \"\n",
    "        \"3) Compute |P - threshold_v3|; if < conf_margin_v3 and dt_high_recall predicts 1, \"\n",
    "        \"   then final label = 1 (upgrade-only).\"\n",
    "    )\n",
    "}\n",
    "\n",
    "config_path = os.path.join(MODEL_DIR, \"hybrid_v3_config.json\")\n",
    "with open(config_path, \"w\") as f:\n",
    "    json.dump(hybrid_v3_config, f, indent=2)\n",
    "\n",
    "print(f\"Saved Hybrid v3 config to: {config_path}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.14.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
